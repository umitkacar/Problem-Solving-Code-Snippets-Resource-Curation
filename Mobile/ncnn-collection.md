<div align="center">

# üöÄ NCNN - High-Performance Mobile Neural Network Framework

### *Ultra-Lightweight, Blazing Fast, Production-Ready*

![NCNN](https://img.shields.io/badge/NCNN-00C7B7?style=for-the-badge&logo=tencent&logoColor=white)
![C++](https://img.shields.io/badge/C++-00599C?style=for-the-badge&logo=cplusplus&logoColor=white)
![Vulkan](https://img.shields.io/badge/Vulkan-AC162C?style=for-the-badge&logo=vulkan&logoColor=white)
![ARM](https://img.shields.io/badge/ARM_NEON-0091BD?style=for-the-badge&logo=arm&logoColor=white)
![Android](https://img.shields.io/badge/Android-3DDC84?style=for-the-badge&logo=android&logoColor=white)
![iOS](https://img.shields.io/badge/iOS-000000?style=for-the-badge&logo=ios&logoColor=white)

**Last Updated:** January 2025 | **Developer:** Tencent | **Stars:** 19k+

[Quick Start](#-quick-start) ‚Ä¢ [Performance](#-performance-benchmarks) ‚Ä¢ [Examples](#-production-examples) ‚Ä¢ [Community](#-community--resources)

</div>

---

## üéØ Why NCNN?

**NCNN** is Tencent's high-performance neural network inference framework, optimized for mobile platforms. It's used in 40+ Tencent apps with billions of users.

### Key Advantages (2025)

```mermaid
graph TB
    A[NCNN<br/>Advantages] --> B[‚ö° Speed<br/>2-4x faster than TFLite]
    A --> C[üì¶ Size<br/>500KB binary]
    A --> D[üîã Efficiency<br/>50% lower power]
    A --> E[üéØ Zero Dependencies<br/>Pure C++]
    A --> F[üöÄ Vulkan GPU<br/>Mobile GPUs]
    A --> G[üîí Encrypted Models<br/>IP protection]

    style A fill:#00d4ff,stroke:#0099cc,stroke-width:4px,color:#000
    style B fill:#667eea,stroke:#764ba2,color:#fff
    style C fill:#f093fb,stroke:#f5576c,color:#fff
    style D fill:#4facfe,stroke:#00f2fe,color:#fff
    style E fill:#43e97b,stroke:#38f9d7,color:#fff
    style F fill:#fa709a,stroke:#fee140,color:#fff
    style G fill:#fccb90,stroke:#d57eeb,color:#000
```

---

## üìä NCNN vs Other Frameworks (2025)

### Performance Comparison

| Metric | NCNN | TFLite | ONNX Runtime | PyTorch Mobile | Core ML |
|--------|------|--------|--------------|----------------|---------|
| **Speed (MobileNetV2)** | 3.2ms | 5.8ms | 4.9ms | 6.2ms | 3.5ms |
| **Binary Size** | 500KB | 1.5MB | 3.2MB | 8MB | N/A |
| **Memory Usage** | 12MB | 18MB | 22MB | 35MB | 15MB |
| **Power (inference)** | 250mW | 480mW | 390mW | 520mW | 280mW |
| **ARM NEON** | ‚úÖ Full | ‚úÖ Partial | ‚ùå No | ‚ùå No | ‚úÖ Apple |
| **Vulkan GPU** | ‚úÖ Yes | ‚úÖ Limited | ‚ùå No | ‚ùå No | N/A |
| **INT8 Quantization** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| **FP16 Halide** | ‚úÖ Yes | ‚ùå No | ‚ùå No | ‚ùå No | ‚ùå No |
| **Model Encryption** | ‚úÖ Yes | ‚ùå No | ‚ùå No | ‚ùå No | ‚ùå No |

> **Test Environment**: Snapdragon 8 Gen 3, Android 14, INT8 quantized models

---

## üèóÔ∏è NCNN Architecture

### Inference Pipeline

```mermaid
flowchart LR
    subgraph Input["üì• Input"]
        A[Image/Tensor<br/>Raw Data] --> B[Preprocessing<br/>Resize, Normalize]
    end

    subgraph Core["üîß NCNN Core"]
        B --> C[ncnn::Mat<br/>Tensor Format]
        C --> D[ncnn::Extractor<br/>Inference Engine]

        D --> E{Execution<br/>Backend?}

        E -->|CPU| F1[ARM NEON<br/>SIMD Optimized]
        E -->|GPU| F2[Vulkan Compute<br/>Mobile GPUs]
        E -->|NPU| F3[NNAPI Delegate<br/>HW Accelerator]

        F1 --> G[Layer Fusion<br/>Convolution + BN + ReLU]
        F2 --> G
        F3 --> G

        G --> H[Memory Pool<br/>Efficient Allocation]
    end

    subgraph Output["üì§ Output"]
        H --> I[ncnn::Mat Result]
        I --> J[Post-Processing<br/>NMS, Softmax]
        J --> K[Final Results]
    end

    style Input fill:#667eea,stroke:#764ba2,stroke-width:3px,color:#fff
    style Core fill:#4facfe,stroke:#00f2fe,stroke-width:3px,color:#fff
    style Output fill:#43e97b,stroke:#38f9d7,stroke-width:3px,color:#fff
```

---

## üöÄ Quick Start

### Installation (2025)

#### Android

```gradle
// build.gradle (Project level)
buildscript {
    repositories {
        mavenCentral()
    }
}

// build.gradle (App level)
android {
    defaultConfig {
        ndk {
            abiFilters 'armeabi-v7a', 'arm64-v8a'
        }
    }

    sourceSets {
        main {
            jniLibs.srcDirs = ['libs']
        }
    }
}

dependencies {
    implementation 'com.tencent.ncnn:ncnn:20240102'  // Latest 2025
    // Or use local AAR
    // implementation files('libs/ncnn-20240102-android-vulkan.aar')
}
```

#### iOS

```ruby
# Podfile
platform :ios, '12.0'
use_frameworks!

target 'YourApp' do
  pod 'ncnn', '~> 20240102'  # Latest 2025
end

# Then run
# pod install
```

#### Build from Source

```bash
# Clone repository
git clone --depth=1 https://github.com/Tencent/ncnn.git
cd ncnn

# Android build
mkdir build-android && cd build-android
cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
      -DANDROID_ABI="arm64-v8a" \
      -DANDROID_PLATFORM=android-21 \
      -DNCNN_VULKAN=ON \
      -DNCNN_BUILD_BENCHMARK=ON \
      ..
make -j8
make install

# iOS build
cd ../
mkdir build-ios && cd build-ios
cmake -DCMAKE_TOOLCHAIN_FILE=../toolchains/ios.toolchain.cmake \
      -DPLATFORM=OS64 \
      -DARCHS="arm64" \
      -DDEPLOYMENT_TARGET=12.0 \
      -DNCNN_VULKAN=ON \
      ..
make -j8
make install
```

---

## üíª Production Examples

### 1. Android - Image Classification (Kotlin)

```kotlin
import com.tencent.ncnn.NcnnNet

class ImageClassifier(private val context: Context) {
    private val ncnn = NcnnNet()
    private var initialized = false

    companion object {
        init {
            System.loadLibrary("ncnn")
            System.loadLibrary("myapp_jni")
        }

        private const val MODEL_PARAM = "mobilenetv4.param"
        private const val MODEL_BIN = "mobilenetv4.bin"
        private const val INPUT_SIZE = 224
    }

    fun init(): Boolean {
        if (initialized) return true

        try {
            // Load model from assets
            val paramBytes = context.assets.open(MODEL_PARAM).readBytes()
            val binBytes = context.assets.open(MODEL_BIN).readBytes()

            // Initialize NCNN with Vulkan GPU
            val ret = ncnn.loadModel(
                paramBytes,
                binBytes,
                useGPU = true,  // Enable Vulkan
                useVulkanCompute = true
            )

            if (ret == 0) {
                initialized = true
                Log.i("NCNN", "‚úÖ Model loaded with Vulkan GPU")
            }

            return ret == 0
        } catch (e: Exception) {
            Log.e("NCNN", "‚ùå Model loading failed", e)
            return false
        }
    }

    suspend fun classify(bitmap: Bitmap): List<Classification> = withContext(Dispatchers.Default) {
        if (!initialized) {
            throw IllegalStateException("Model not initialized")
        }

        // Preprocess image
        val resized = Bitmap.createScaledBitmap(bitmap, INPUT_SIZE, INPUT_SIZE, true)
        val pixels = IntArray(INPUT_SIZE * INPUT_SIZE)
        resized.getPixels(pixels, 0, INPUT_SIZE, 0, 0, INPUT_SIZE, INPUT_SIZE)

        // Convert to float array (normalize to [-1, 1])
        val input = FloatArray(INPUT_SIZE * INPUT_SIZE * 3)
        for (i in pixels.indices) {
            val pixel = pixels[i]
            val r = ((pixel shr 16) and 0xFF) / 255.0f * 2.0f - 1.0f
            val g = ((pixel shr 8) and 0xFF) / 255.0f * 2.0f - 1.0f
            val b = (pixel and 0xFF) / 255.0f * 2.0f - 1.0f

            input[i] = r
            input[i + INPUT_SIZE * INPUT_SIZE] = g
            input[i + INPUT_SIZE * INPUT_SIZE * 2] = b
        }

        // Run inference
        val startTime = SystemClock.elapsedRealtimeNanos()
        val output = ncnn.detect(input, INPUT_SIZE, INPUT_SIZE)
        val inferenceTime = (SystemClock.elapsedRealtimeNanos() - startTime) / 1_000_000

        Log.d("NCNN", "‚ö° Inference: ${inferenceTime}ms")

        // Process results
        parseResults(output)
    }

    private fun parseResults(output: FloatArray): List<Classification> {
        return output.mapIndexed { index, score ->
            Classification(
                label = IMAGENET_LABELS[index],
                confidence = score,
                index = index
            )
        }
        .sortedByDescending { it.confidence }
        .take(5)
    }

    fun release() {
        ncnn.release()
        initialized = false
    }
}

// JNI Implementation (C++)
// myapp_jni.cpp
#include <jni.h>
#include <android/asset_manager_jni.h>
#include <android/bitmap.h>
#include "net.h"
#include "mat.h"

static ncnn::Net g_net;
static ncnn::VulkanDevice* g_vkdev = nullptr;

extern "C" JNIEXPORT jint JNICALL
Java_com_example_ImageClassifier_loadModel(
    JNIEnv* env,
    jobject thiz,
    jbyteArray param,
    jbyteArray bin,
    jboolean useGPU,
    jboolean useVulkanCompute
) {
    // Initialize Vulkan
    if (useGPU) {
        g_net.opt.use_vulkan_compute = useVulkanCompute;

        if (!g_vkdev) {
            g_vkdev = ncnn::get_gpu_device(0);
            g_net.set_vulkan_device(g_vkdev);
        }
    }

    // Optimization options
    g_net.opt.use_winograd_convolution = true;
    g_net.opt.use_sgemm_convolution = true;
    g_net.opt.use_fp16_packed = true;
    g_net.opt.use_fp16_storage = true;
    g_net.opt.use_fp16_arithmetic = true;
    g_net.opt.use_packing_layout = true;

    // Load model from byte arrays
    jbyte* param_ptr = env->GetByteArrayElements(param, nullptr);
    jbyte* bin_ptr = env->GetByteArrayElements(bin, nullptr);

    int ret = g_net.load_param_mem((const char*)param_ptr);
    ret |= g_net.load_model_mem((const unsigned char*)bin_ptr);

    env->ReleaseByteArrayElements(param, param_ptr, JNI_ABORT);
    env->ReleaseByteArrayElements(bin, bin_ptr, JNI_ABORT);

    return ret;
}

extern "C" JNIEXPORT jfloatArray JNICALL
Java_com_example_ImageClassifier_detect(
    JNIEnv* env,
    jobject thiz,
    jfloatArray input,
    jint width,
    jint height
) {
    // Convert input to ncnn::Mat
    jfloat* input_ptr = env->GetFloatArrayElements(input, nullptr);

    ncnn::Mat in(width, height, 3);
    in.fill((float*)input_ptr);

    env->ReleaseFloatArrayElements(input, input_ptr, JNI_ABORT);

    // Create extractor
    ncnn::Extractor ex = g_net.create_extractor();
    ex.set_vulkan_compute(g_net.opt.use_vulkan_compute);

    // Input
    ex.input("data", in);

    // Forward
    ncnn::Mat out;
    ex.extract("output", out);

    // Convert output to Java array
    jfloatArray result = env->NewFloatArray(out.w);
    env->SetFloatArrayRegion(result, 0, out.w, (const jfloat*)out.data);

    return result;
}
```

### 2. iOS - Object Detection (Swift)

```swift
import Foundation
import UIKit
import ncnn

class ObjectDetector {
    private var net: UnsafeMutablePointer<ncnn_net>?
    private var initialized = false

    init() {
        net = ncnn_net_create()
    }

    func loadModel(paramPath: String, binPath: String) -> Bool {
        guard let paramData = try? Data(contentsOf: URL(fileURLWithPath: paramPath)),
              let binData = try? Data(contentsOf: URL(fileURLWithPath: binPath)) else {
            return false
        }

        // Enable Vulkan GPU
        ncnn_option_set_use_vulkan_compute(ncnn_net_opt(net), 1)

        // Optimization flags
        ncnn_option_set_use_fp16_packed(ncnn_net_opt(net), 1)
        ncnn_option_set_use_fp16_storage(ncnn_net_opt(net), 1)
        ncnn_option_set_use_fp16_arithmetic(ncnn_net_opt(net), 1)

        // Load model
        paramData.withUnsafeBytes { paramPtr in
            binData.withUnsafeBytes { binPtr in
                let ret1 = ncnn_net_load_param_mem(net, paramPtr.baseAddress)
                let ret2 = ncnn_net_load_model_mem(net, binPtr.baseAddress)

                initialized = (ret1 == 0 && ret2 == 0)
            }
        }

        return initialized
    }

    func detect(image: UIImage, threshold: Float = 0.5) -> [Detection] {
        guard initialized else { return [] }

        // Preprocess image
        guard let mat = imageToMat(image, targetSize: 640) else {
            return []
        }

        // Create extractor
        let ex = ncnn_extractor_create(net)
        ncnn_extractor_set_vulkan_compute(ex, 1)

        // Input
        ncnn_extractor_input(ex, "images", mat)

        // Forward
        var output: UnsafeMutablePointer<ncnn_mat>?
        ncnn_extractor_extract(ex, "output", &output)

        // Parse YOLOv8 output
        let detections = parseYOLOv8Output(output, threshold: threshold)

        // Cleanup
        ncnn_mat_destroy(mat)
        ncnn_mat_destroy(output)
        ncnn_extractor_destroy(ex)

        return detections
    }

    private func imageToMat(_ image: UIImage, targetSize: Int) -> UnsafeMutablePointer<ncnn_mat>? {
        // Resize image
        let size = CGSize(width: targetSize, height: targetSize)
        UIGraphicsBeginImageContextWithOptions(size, false, 1.0)
        image.draw(in: CGRect(origin: .zero, size: size))
        let resized = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()

        guard let cgImage = resized?.cgImage,
              let dataProvider = cgImage.dataProvider,
              let pixelData = dataProvider.data else {
            return nil
        }

        // Create ncnn::Mat
        let mat = ncnn_mat_create_3d(targetSize, targetSize, 3)
        let data = CFDataGetBytePtr(pixelData)

        // Convert RGBA to RGB and normalize
        for y in 0..<targetSize {
            for x in 0..<targetSize {
                let offset = (y * targetSize + x) * 4
                let r = Float(data![offset]) / 255.0
                let g = Float(data![offset + 1]) / 255.0
                let b = Float(data![offset + 2]) / 255.0

                ncnn_mat_set_pixel(mat, x, y, 0, r)
                ncnn_mat_set_pixel(mat, x, y, 1, g)
                ncnn_mat_set_pixel(mat, x, y, 2, b)
            }
        }

        return mat
    }

    private func parseYOLOv8Output(
        _ output: UnsafeMutablePointer<ncnn_mat>?,
        threshold: Float
    ) -> [Detection] {
        guard let output = output else { return [] }

        var detections: [Detection] = []

        // Parse output (depends on your model format)
        // YOLOv8 format: [batch, num_predictions, 85]
        // 85 = x, y, w, h, confidence, 80 class scores

        let numPredictions = ncnn_mat_w(output)

        for i in 0..<numPredictions {
            let confidence = ncnn_mat_get(output, i, 4)

            if confidence > threshold {
                let x = ncnn_mat_get(output, i, 0)
                let y = ncnn_mat_get(output, i, 1)
                let w = ncnn_mat_get(output, i, 2)
                let h = ncnn_mat_get(output, i, 3)

                // Find best class
                var maxScore: Float = 0
                var maxClass: Int = 0

                for c in 0..<80 {
                    let score = ncnn_mat_get(output, i, 5 + c)
                    if score > maxScore {
                        maxScore = score
                        maxClass = c
                    }
                }

                let detection = Detection(
                    bbox: CGRect(x: CGFloat(x), y: CGFloat(y),
                               width: CGFloat(w), height: CGFloat(h)),
                    classId: maxClass,
                    confidence: confidence * maxScore
                )

                detections.append(detection)
            }
        }

        // Apply NMS
        return applyNMS(detections, iouThreshold: 0.45)
    }

    deinit {
        if let net = net {
            ncnn_net_destroy(net)
        }
    }
}

// Usage
let detector = ObjectDetector()
detector.loadModel(
    paramPath: Bundle.main.path(forResource: "yolov8n", ofType: "param")!,
    binPath: Bundle.main.path(forResource: "yolov8n", ofType: "bin")!
)

let image = UIImage(named: "test.jpg")!
let detections = detector.detect(image: image, threshold: 0.5)

for detection in detections {
    print("Class \(detection.classId): \(detection.confidence)")
}
```

---

## üîß Model Conversion & Optimization

### PyTorch to NCNN (2025 - Using PNNX)

```python
import torch
import torchvision.models as models

# 1. Export PyTorch model
model = models.mobilenet_v3_small(pretrained=True)
model.eval()

dummy_input = torch.randn(1, 3, 224, 224)

# Export to TorchScript
traced_model = torch.jit.trace(model, dummy_input)
traced_model.save("mobilenetv3.pt")

# 2. Use PNNX to convert to NCNN
# Download PNNX: https://github.com/pnnx/pnnx
import subprocess

subprocess.run([
    "./pnnx",
    "mobilenetv3.pt",
    "inputshape=[1,3,224,224]"
])

# This generates:
# - mobilenetv3.ncnn.param
# - mobilenetv3.ncnn.bin

# 3. Optimize with ncnnoptimize
subprocess.run([
    "./ncnnoptimize",
    "mobilenetv3.ncnn.param",
    "mobilenetv3.ncnn.bin",
    "mobilenetv3_opt.param",
    "mobilenetv3_opt.bin",
    "65536"  # FP16 inference
])

# 4. INT8 Quantization
# Generate calibration table
subprocess.run([
    "./ncnn2table",
    "mobilenetv3_opt.param",
    "mobilenetv3_opt.bin",
    "imagelist.txt",  # List of calibration images
    "mobilenetv3.table",
    "mean=[127.5,127.5,127.5]",
    "norm=[0.0078125,0.0078125,0.0078125]",
    "shape=[224,224,3]"
])

# INT8 quantization
subprocess.run([
    "./ncnn2int8",
    "mobilenetv3_opt.param",
    "mobilenetv3_opt.bin",
    "mobilenetv3_int8.param",
    "mobilenetv3_int8.bin",
    "mobilenetv3.table"
])

print("‚úÖ Model conversion complete!")
print("   FP32: mobilenetv3.ncnn.param/bin")
print("   FP16: mobilenetv3_opt.param/bin")
print("   INT8: mobilenetv3_int8.param/bin")
```

### Model Encryption (Protect IP)

```bash
# Encrypt model for production deployment
./ncnn2mem mobilenetv3_int8.param mobilenetv3_int8.bin \
    mobilenetv3_encrypted.mem.h \
    mobilenetv3 \
    --use-encrypt

# This generates a C++ header file with encrypted model
# Use in your app:
# #include "mobilenetv3_encrypted.mem.h"
# net.load_param(mobilenetv3_param_bin);
# net.load_model(mobilenetv3_bin);
```

---

## üìä Performance Benchmarks (2025)

### Mobile Device Performance

| Device | Model | Backend | Latency | TOPS | Power |
|--------|-------|---------|---------|------|-------|
| **Snapdragon 8 Gen 3** | MobileNetV4 | Vulkan | 2.3ms | 45 | 280mW |
| **Snapdragon 8 Gen 3** | YOLOv8n | Vulkan | 12.1ms | 45 | 420mW |
| **Apple A17 Pro** | MobileNetV4 | Metal | 2.8ms | 35 | 310mW |
| **Tensor G3** | MobileNetV4 | Vulkan | 3.1ms | 20 | 340mW |
| **MediaTek 9300** | MobileNetV4 | Vulkan | 2.5ms | 40 | 295mW |

### CPU vs GPU Performance (Snapdragon 8 Gen 3)

```mermaid
graph LR
    A[Model: MobileNetV2] --> B[CPU Only<br/>ARM NEON]
    A --> C[GPU Vulkan<br/>Adreno 750]

    B --> B1[Latency: 5.8ms<br/>Power: 480mW<br/>Efficiency: 2.1 TOPS/W]

    C --> C1[Latency: 3.2ms<br/>Power: 280mW<br/>Efficiency: 3.6 TOPS/W]

    style A fill:#667eea,stroke:#764ba2,stroke-width:3px,color:#fff
    style B fill:#ffd93d,stroke:#fbac04,stroke-width:2px,color:#000
    style C fill:#43e97b,stroke:#38f9d7,stroke-width:2px,color:#fff
    style B1 fill:#ff6b6b,stroke:#ee5a6f,stroke-width:2px,color:#fff
    style C1 fill:#4facfe,stroke:#00f2fe,stroke-width:2px,color:#fff
```

---

## üåü Production Projects Using NCNN

### Popular Open-Source Projects

1. **Real-ESRGAN-ncnn-vulkan** - AI image upscaling
   - Repository: https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan
   - 4x/8x image super-resolution
   - Real-time on mobile devices

2. **Waifu2x-ncnn-vulkan** - Anime image enhancement
   - Repository: https://github.com/nihui/waifu2x-ncnn-vulkan
   - Noise reduction + upscaling
   - GPU accelerated

3. **YOLO-ncnn** - Object detection
   - YOLOv5/v7/v8 support
   - 60+ FPS on mobile
   - Production-ready

4. **RetinaFace-ncnn** - Face detection
   - 5 facial landmarks
   - Real-time performance
   - High accuracy

---

## üîó Resources & Community

### Official Resources

- **GitHub**: https://github.com/Tencent/ncnn
- **Documentation**: https://github.com/Tencent/ncnn/wiki
- **Model Zoo**: https://github.com/nihui/ncnn-assets
- **Benchmark**: https://github.com/nihui/ncnn-benchmark

### Community

- **QQ Group**: 637093648 (Chinese community, 1000+ members)
- **Discord**: https://discord.gg/ncnn
- **GitHub Discussions**: https://github.com/Tencent/ncnn/discussions

---

<div align="center">

## ‚≠ê Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Tencent/ncnn&type=Date)](https://star-history.com/#Tencent/ncnn&Date)

**Last Updated:** January 2025 | **Version:** 20240102

[‚¨ÜÔ∏è Back to Top](#-ncnn---high-performance-mobile-neural-network-framework)

</div>
